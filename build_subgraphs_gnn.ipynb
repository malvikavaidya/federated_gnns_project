{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 746])\n",
      "torch.Size([2, 164])\n",
      "torch.Size([2, 570])\n",
      "torch.Size([2, 92])\n",
      "torch.Size([2, 228])\n",
      "torch.Size([2, 854])\n",
      "torch.Size([2, 136])\n",
      "torch.Size([2, 378])\n",
      "torch.Size([2, 102])\n",
      "torch.Size([2, 856])\n",
      "torch.Size([2, 144])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 734])\n",
      "torch.Size([2, 132])\n",
      "torch.Size([2, 862])\n",
      "torch.Size([2, 100])\n",
      "torch.Size([2, 136])\n",
      "torch.Size([2, 418])\n",
      "torch.Size([2, 72])\n",
      "torch.Size([2, 98])\n",
      "torch.Size([2, 142])\n",
      "torch.Size([2, 148])\n",
      "torch.Size([2, 238])\n",
      "torch.Size([2, 92])\n",
      "torch.Size([2, 124])\n",
      "torch.Size([2, 368])\n",
      "torch.Size([2, 350])\n",
      "torch.Size([2, 414])\n",
      "torch.Size([2, 1230])\n",
      "torch.Size([2, 184])\n",
      "torch.Size([2, 756])\n",
      "torch.Size([2, 1790])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([2, 182])\n",
      "torch.Size([2, 156])\n",
      "torch.Size([2, 82])\n",
      "torch.Size([2, 166])\n",
      "torch.Size([2, 376])\n",
      "torch.Size([2, 330])\n",
      "torch.Size([2, 178])\n",
      "torch.Size([2, 120])\n",
      "torch.Size([2, 144])\n",
      "torch.Size([2, 160])\n",
      "torch.Size([2, 340])\n",
      "torch.Size([2, 148])\n",
      "torch.Size([2, 220])\n",
      "torch.Size([2, 790])\n",
      "torch.Size([2, 92])\n",
      "torch.Size([2, 136])\n",
      "torch.Size([2, 616])\n",
      "torch.Size([2, 228])\n",
      "torch.Size([2, 246])\n",
      "torch.Size([2, 1462])\n",
      "torch.Size([2, 928])\n",
      "torch.Size([2, 774])\n",
      "torch.Size([2, 118])\n",
      "torch.Size([2, 100])\n",
      "torch.Size([2, 400])\n",
      "torch.Size([2, 222])\n",
      "torch.Size([2, 360])\n",
      "torch.Size([2, 124])\n",
      "torch.Size([2, 776])\n",
      "torch.Size([2, 292])\n",
      "torch.Size([2, 746])\n",
      "torch.Size([2, 304])\n",
      "torch.Size([2, 570])\n",
      "torch.Size([2, 1078])\n",
      "torch.Size([2, 194])\n",
      "torch.Size([2, 890])\n",
      "torch.Size([2, 192])\n",
      "torch.Size([2, 696])\n",
      "torch.Size([2, 528])\n",
      "torch.Size([2, 250])\n",
      "torch.Size([2, 164])\n",
      "torch.Size([2, 200])\n",
      "torch.Size([2, 138])\n",
      "torch.Size([2, 386])\n",
      "torch.Size([2, 144])\n",
      "torch.Size([2, 148])\n",
      "torch.Size([2, 544])\n",
      "torch.Size([2, 834])\n",
      "torch.Size([2, 296])\n",
      "torch.Size([2, 374])\n",
      "torch.Size([2, 460])\n",
      "torch.Size([2, 618])\n",
      "torch.Size([2, 264])\n",
      "torch.Size([2, 566])\n",
      "torch.Size([2, 316])\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 776])\n",
      "torch.Size([2, 98])\n",
      "torch.Size([2, 756])\n",
      "torch.Size([2, 730])\n",
      "torch.Size([2, 242])\n",
      "torch.Size([2, 216])\n",
      "torch.Size([2, 158])\n",
      "torch.Size([2, 134])\n",
      "torch.Size([2, 190])\n",
      "torch.Size([2, 252])\n",
      "torch.Size([2, 774])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234567)\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "folder_path = 'second_order_client_neighbors'\n",
    "sub_data_list = []\n",
    "client_number = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.gml'):\n",
    "      \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        g = nx.read_gml(file_path)\n",
    "\n",
    "        subgraph_nodes = list(g.nodes)\n",
    "        subgraph_nodes = [int(node) for node in subgraph_nodes]  # Convert to integer if they are not\n",
    "\n",
    "        sub_edge_index, _ = subgraph(subgraph_nodes, data.edge_index, relabel_nodes=True)\n",
    "        print(sub_edge_index.shape)\n",
    "\n",
    "        sub_data = Data(x=data.x[subgraph_nodes], edge_index=sub_edge_index, y=data.y[subgraph_nodes])\n",
    "        sub_data_list.append(sub_data)\n",
    "        client_number.append(int(filename.split('.')[0].split('_')[1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = None\n",
    "\n",
    "g = nx.read_gml('second_order_test_graph.gml')\n",
    "subgraph_nodes = list(g.nodes)\n",
    "subgraph_nodes = [int(node) for node in subgraph_nodes]  # Convert to integer if they are not\n",
    "sub_edge_index, _ = subgraph(subgraph_nodes, data.edge_index, relabel_nodes=True)\n",
    "test_data = Data(x=data.x[subgraph_nodes], edge_index=sub_edge_index, y=data.y[subgraph_nodes])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'828': ['713', '705', '719', '805', '824', '745', '747', '823', '694', '830', '781', '697', '724', '827', '688', '853', '834', '703', '784', '815', '752', '728', '820', '842', '800', '819', '774', '726', '773', '766', '829', '780', '810', '696', '849', '718', '754', '760', '739', '856', '764', '687', '840', '847', '770', '797', '814', '727', '731', '708', '698', '817', '792', '845', '838', '848', '835', '807', '711', '779', '706', '772', '741', '709', '778', '737', '844', '693'], '713': ['828', '705', '719', '805', '824', '745', '747', '823', '694', '830', '781', '697', '724', '827', '688', '853', '834', '695', '784', '815', '752', '842', '800', '819', '774', '726', '773', '766', '829', '780', '810', '696', '760', '787', '840', '847', '814', '731', '708', '698', '738', '817', '734', '845', '701', '848', '748', '706', '777', '772', '818', '778', '831', '833', '722', '844', '736', '794', '693'], '705': ['828', '713', '719', '805', '824', '745', '747', '823', '694', '830', '781', '697', '724', '827', '853', '834', '703', '784', '752', '728', '820', '842', '800', '819', '726', '773', '829', '780', '810', '849', '718', '754', '760', '739', '787', '764', '847', '770', '814', '731', '738', '845', '835', '807', '758', '803', '748', '755', '809', '720', '706', '772', '741', '778', '737', '831', '722', '844', '693', '763'], '719': ['828', '713', '705', '805', '824', '745', '747', '823', '694', '830', '781', '697', '724', '827', '688', '853', '834', '703', '695', '784', '815', '752', '728', '820', '842', '800', '819', '774', '726', '773', '766', '829', '810', '696', '849', '718', '760', '787', '764', '840', '847', '814', '727', '731', '708', '698', '817', '734', '845', '848', '755', '720', '706', '772', '737', '722', '768', '794'], '805': ['828', '713', '705', '719', '824', '745', '747', '823', '694', '781', '697', '724', '827', '688', '853', '834', '703', '695', '815', '752', '728', '842', '800', '819', '774', '726', '766', '829', '780', '810', '696', '718', '754', '760', '739', '764', '687', '840', '847', '814', '727', '731', '738', '817', '792', '734', '838', '807', '706', '772', '709', '818', '737', '831', '844'], '824': ['828', '713', '705', '719', '805', '745', '823', '694', '830', '781', '697', '724', '827', '688', '853', '834', '703', '695', '784', '815', '752', '728', '820', '842', '819', '774', '726', '773', '766', '829', '780', '696', '849', '754', '760', '739', '764', '840', '847', '770', '727', '708', '792', '838', '848', '835', '779', '777', '709', '737', '844', '693'], '745': ['828', '713', '705', '719', '805', '824', '747', '823', '694', '830', '781', '697', '724', '827', '853', '834', '695', '784', '815', '752', '820', '842', '800', '774', '726', '773', '829', '810', '696', '718', '760', '787', '847', '814', '731', '698', '738', '817', '845', '758', '711', '755', '720', '706', '795', '737', '722', '794'], '747': ['828', '713', '705', '719', '805', '745', '823', '830', '697', '688', '703', '815', '752', '728', '820', '800', '819', '774', '773', '780', '810', '696', '849', '718', '754', '739', '856', '764', '687', '840', '770', '797', '708', '698', '792', '838', '701', '848', '807', '793', '758', '803', '748', '783', '809', '741', '831', '730', '763', '765'], '823': ['828', '713', '705', '719', '805', '824', '745', '747', '694', '830', '781', '697', '724', '827', '688', '853', '834', '703', '815', '752', '728', '820', '800', '819', '774', '726', '773', '766', '780', '810', '696', '718', '754', '760', '739', '787', '856', '764', '687', '840', '708', '698', '792', '838', '848', '835', '807', '779', '741', '709', '693', '765'], '694': ['828', '713', '705', '719', '805', '824', '745', '823', '830', '781', '697', '724', '827', '688', '853', '834', '703', '695', '784', '815', '752', '820', '842', '800', '774', '726', '773', '766', '829', '780', '810', '696', '754', '760', '847', '727', '731', '738', '845', '848', '711', '706', '795', '772', '737', '831', '722'], '830': ['828', '713', '705', '719', '824', '745', '747', '823', '694', '781', '697', '724', '827', '688', '853', '834', '695', '815', '752', '820', '800', '819', '774', '726', '766', '780', '696', '849', '754', '760', '856', '687', '840', '847', '727', '731', '817', '792', '845', '838', '701', '748', '777', '709', '778', '737', '844'], '781': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '697', '724', '827', '853', '834', '703', '695', '784', '815', '752', '728', '842', '800', '726', '773', '829', '810', '849', '718', '760', '787', '764', '770', '814', '731', '708', '738', '817', '701', '793', '758', '755', '783', '809', '720', '772', '741', '794'], '697': ['828', '713', '705', '719', '805', '824', '745', '747', '823', '694', '830', '781', '724', '827', '688', '834', '695', '815', '820', '842', '800', '819', '726', '766', '780', '696', '849', '754', '760', '856', '687', '840', '727', '731', '708', '698', '701', '848', '835', '748', '777', '772', '709', '818', '844', '693', '765'], '724': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '697', '827', '688', '853', '834', '695', '784', '752', '820', '842', '819', '726', '773', '766', '829', '696', '754', '760', '814', '731', '708', '738', '734', '701', '848', '711', '795', '777', '778', '736', '794'], '827': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '697', '724', '853', '834', '703', '784', '752', '842', '800', '819', '773', '766', '810', '849', '718', '760', '787', '764', '797', '814', '731', '708', '817', '758', '755', '783', '809', '772', '741'], '688': ['828', '713', '719', '805', '824', '747', '823', '694', '830', '697', '724', '853', '834', '695', '815', '752', '800', '819', '774', '726', '766', '780', '696', '849', '754', '739', '856', '687', '840', '847', '770', '727', '792', '734', '845', '701', '777', '772', '771'], '853': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '724', '827', '688', '834', '695', '784', '815', '752', '820', '842', '800', '774', '726', '773', '766', '829', '810', '696', '847', '814', '727', '731', '738', '734', '711', '706', '795', '831', '722'], '834': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '697', '724', '827', '688', '853', '695', '784', '815', '752', '820', '842', '800', '819', '726', '773', '766', '829', '780', '810', '696', '718', '760', '847', '814', '731', '701', '706', '778', '844', '693'], '703': ['828', '705', '719', '805', '824', '747', '823', '694', '781', '827', '815', '728', '820', '819', '718', '754', '739', '787', '856', '764', '687', '797', '698', '817', '792', '845', '838', '835', '807', '793', '758', '748', '779', '809', '741', '709', '831', '730', '765'], '695': ['713', '719', '805', '824', '745', '694', '830', '781', '697', '724', '688', '853', '834', '784', '815', '752', '842', '726', '766', '829', '780', '810', '696', '718', '754', '760', '840', '847', '738', '734', '701', '711', '706', '777', '778', '844', '794', '693'], '784': ['828', '713', '705', '719', '824', '745', '694', '781', '724', '827', '853', '834', '695', '752', '842', '800', '774', '726', '773', '766', '829', '810', '696', '760', '727', '731', '738', '734', '711', '706', '795', '722', '794'], '815': ['828', '713', '719', '805', '824', '745', '747', '823', '694', '830', '781', '697', '688', '853', '834', '703', '695', '752', '819', '726', '773', '766', '780', '810', '718', '754', '856', '847', '731', '792', '845', '838', '835', '706', '737'], '752': ['828', '713', '705', '719', '805', '824', '745', '747', '823', '694', '830', '781', '724', '827', '688', '853', '834', '695', '784', '815', '820', '842', '800', '774', '726', '773', '766', '829', '810', '847', '727', '731', '738', '711', '706', '831'], '728': ['828', '705', '719', '805', '824', '747', '823', '781', '703', '820', '819', '774', '773', '780', '754', '739', '787', '856', '764', '687', '797', '817', '792', '838', '835', '807', '803', '783', '779', '809', '741', '737', '730', '765'], '820': ['828', '705', '719', '824', '745', '747', '823', '694', '830', '697', '724', '853', '834', '703', '752', '728', '774', '726', '773', '766', '849', '760', '687', '770', '797', '845', '848', '803', '783', '795', '741', '737', '831', '722', '763'], '842': ['828', '713', '705', '719', '805', '824', '745', '694', '781', '697', '724', '827', '853', '834', '695', '784', '752', '800', '773', '829', '810', '696', '760', '814', '727', '738', '734', '701', '711', '795', '794'], '800': ['828', '713', '705', '719', '805', '745', '747', '823', '694', '830', '781', '697', '827', '688', '853', '834', '784', '752', '842', '774', '766', '810', '760', '856', '814', '727', '708', '698', '845', '772', '741'], '819': ['828', '713', '705', '719', '805', '824', '747', '823', '830', '697', '724', '827', '688', '834', '703', '815', '728', '774', '780', '739', '856', '687', '840', '708', '698', '817', '792', '838', '848', '835', '807', '709', '778'], '774': ['828', '713', '719', '805', '824', '745', '747', '823', '694', '830', '688', '853', '784', '752', '728', '820', '800', '819', '773', '810', '696', '739', '856', '687', '840', '731', '708', '698', '738', '701', '807', '720'], '726': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '697', '724', '688', '853', '834', '695', '784', '815', '752', '820', '773', '766', '780', '754', '760', '731', '845', '701', '778', '844', '693'], '773': ['828', '713', '705', '719', '824', '745', '747', '823', '694', '781', '724', '827', '853', '834', '784', '815', '752', '728', '820', '842', '774', '726', '829', '780', '810', '696', '754', '847', '731', '706', '737', '831'], '766': ['828', '713', '719', '805', '824', '823', '694', '830', '697', '724', '827', '688', '853', '834', '695', '784', '815', '752', '820', '800', '726', '780', '810', '849', '760', '847', '727', '731', '734', '711', '771'], '829': ['828', '713', '705', '719', '805', '824', '745', '694', '781', '724', '853', '834', '695', '784', '752', '842', '773', '696', '760', '847', '814', '738', '734', '711', '795', '777', '822', '722'], '780': ['828', '713', '705', '805', '824', '747', '823', '694', '830', '697', '688', '834', '695', '815', '728', '819', '726', '773', '766', '696', '754', '739', '687', '840', '792', '845', '838', '701', '835', '803', '779', '777', '765'], '810': ['828', '713', '705', '719', '805', '745', '747', '823', '694', '781', '827', '853', '834', '695', '784', '815', '752', '842', '800', '774', '773', '766', '856', '847', '814', '731', '698', '711', '706'], '696': ['828', '713', '719', '805', '824', '745', '747', '823', '694', '830', '697', '724', '688', '853', '834', '695', '784', '842', '774', '773', '829', '780', '739', '840', '727', '738', '845', '711', '795', '778'], '849': ['828', '705', '719', '824', '747', '830', '781', '697', '827', '688', '820', '766', '787', '770', '797', '727', '708', '734', '845', '793', '758', '748', '755', '783', '730', '763'], '718': ['828', '705', '719', '805', '745', '747', '823', '781', '827', '834', '703', '695', '815', '787', '764', '687', '770', '797', '814', '817', '793', '758', '803', '748', '809', '720', '730'], '754': ['828', '705', '805', '824', '747', '823', '694', '830', '697', '724', '688', '703', '695', '815', '728', '726', '773', '780', '739', '840', '792', '838', '701', '848', '835', '779', '777', '709'], '760': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '697', '724', '827', '834', '695', '784', '820', '842', '800', '726', '766', '829', '814', '708', '738', '734', '701', '794'], '739': ['828', '705', '805', '824', '747', '823', '688', '703', '728', '819', '774', '780', '696', '754', '787', '764', '687', '840', '797', '792', '807', '793', '758', '720', '777', '709'], '787': ['713', '705', '719', '745', '823', '781', '827', '703', '728', '849', '718', '739', '764', '797', '814', '817', '793', '758', '803', '748', '755', '783', '809', '720', '730', '763'], '856': ['828', '747', '823', '830', '697', '688', '703', '815', '728', '800', '819', '774', '810', '687', '840', '708', '698', '838', '807', '772', '765'], '764': ['828', '705', '719', '805', '824', '747', '823', '781', '827', '703', '728', '718', '739', '787', '687', '797', '817', '838', '807', '793', '748', '783', '779', '809', '720', '730'], '687': ['828', '805', '747', '823', '830', '697', '688', '703', '728', '820', '819', '774', '780', '718', '739', '856', '764', '797', '817', '838', '807', '783', '779', '777', '730', '765'], '840': ['828', '713', '719', '805', '824', '747', '823', '830', '697', '688', '695', '819', '774', '780', '696', '754', '739', '856', '708', '698', '838', '848', '803', '779', '709'], '847': ['828', '713', '705', '719', '805', '824', '745', '694', '830', '688', '853', '834', '695', '815', '752', '773', '766', '829', '810', '727', '731', '734', '711', '706'], '770': ['828', '705', '824', '747', '781', '688', '820', '849', '718', '797', '727', '734', '845', '803', '748', '783', '777', '772', '741', '763'], '797': ['828', '747', '827', '703', '728', '820', '849', '718', '739', '787', '764', '687', '770', '708', '817', '793', '748', '755', '783', '809', '831', '730', '763'], '814': ['828', '713', '705', '719', '805', '745', '781', '724', '827', '853', '834', '842', '800', '829', '810', '718', '760', '787', '708', '758', '755', '772'], '727': ['828', '719', '805', '824', '694', '830', '697', '688', '853', '784', '752', '842', '800', '766', '696', '849', '847', '770', '845', '848', '711'], '731': ['828', '713', '705', '719', '805', '745', '694', '830', '781', '697', '724', '827', '853', '834', '784', '815', '752', '774', '726', '773', '766', '810', '847', '706'], '708': ['828', '713', '719', '824', '747', '823', '781', '697', '724', '827', '800', '819', '774', '849', '760', '856', '840', '797', '814', '755', '730'], '698': ['828', '713', '719', '745', '747', '823', '697', '703', '800', '819', '774', '810', '856', '840', '803', '772'], '738': ['713', '705', '805', '745', '694', '781', '724', '853', '695', '784', '752', '842', '774', '829', '696', '760', '711', '795', '722', '794'], '817': ['828', '713', '719', '805', '745', '830', '781', '827', '703', '728', '819', '718', '787', '764', '687', '797', '793', '758', '748', '783', '809'], '792': ['828', '805', '824', '747', '823', '830', '688', '703', '815', '728', '819', '780', '754', '739', '838', '835', '807', '803', '779', '765'], '734': ['713', '719', '805', '724', '688', '853', '695', '784', '842', '766', '829', '849', '760', '847', '770', '803', '778', '693'], '845': ['828', '713', '705', '719', '745', '694', '830', '688', '703', '815', '820', '800', '726', '780', '696', '849', '770', '727', '701', '755', '772'], '838': ['828', '805', '824', '747', '823', '830', '703', '815', '728', '819', '780', '754', '856', '764', '687', '840', '792', '835', '807', '779', '709'], '701': ['713', '747', '830', '781', '697', '724', '688', '834', '695', '842', '774', '726', '780', '754', '760', '845', '777', '778', '794'], '848': ['828', '713', '719', '824', '747', '823', '694', '697', '724', '820', '819', '754', '840', '727', '709', '844'], '835': ['828', '705', '824', '823', '697', '703', '815', '728', '819', '780', '754', '792', '838', '779', '741', '709', '765'], '807': ['828', '705', '805', '747', '823', '703', '728', '819', '774', '739', '856', '764', '687', '792', '838', '779', '741', '765'], '793': ['747', '781', '703', '849', '718', '739', '787', '764', '797', '817', '758', '748', '755', '783', '809', '720'], '758': ['705', '745', '747', '781', '827', '703', '849', '718', '739', '787', '814', '817', '793', '748', '755', '783', '809'], '711': ['828', '745', '694', '724', '853', '695', '784', '752', '842', '766', '829', '810', '696', '847', '727', '738', '795', '722'], '803': ['705', '747', '728', '820', '780', '718', '787', '840', '770', '698', '792', '734', '720', '730', '763'], '748': ['713', '705', '747', '830', '697', '703', '849', '718', '787', '764', '770', '797', '817', '793', '758', '779', '831'], '755': ['705', '719', '745', '781', '827', '849', '787', '797', '814', '708', '845', '793', '758', '783', '809', '720'], '783': ['747', '781', '827', '728', '820', '849', '787', '764', '687', '770', '797', '817', '793', '758', '755', '809', '730'], '779': ['828', '824', '823', '703', '728', '780', '754', '764', '687', '840', '792', '838', '835', '807', '748', '777', '765'], '809': ['705', '747', '781', '827', '703', '728', '718', '787', '764', '797', '817', '793', '758', '755', '783', '720'], '720': ['705', '719', '745', '781', '774', '718', '739', '787', '764', '793', '803', '755', '809', '730'], '706': ['828', '713', '705', '719', '805', '745', '694', '853', '834', '695', '784', '815', '752', '773', '810', '847', '731'], '795': ['745', '694', '724', '853', '784', '820', '842', '829', '696', '738', '711', '794'], '777': ['713', '824', '830', '697', '724', '688', '695', '829', '780', '754', '739', '687', '770', '701', '779', '709'], '772': ['828', '713', '705', '719', '805', '694', '781', '697', '827', '688', '800', '856', '770', '814', '698', '845'], '741': ['828', '705', '747', '823', '781', '827', '703', '728', '820', '800', '770', '835', '807', '763'], '709': ['828', '805', '824', '823', '830', '697', '703', '819', '754', '739', '840', '838', '848', '835', '777'], '818': ['713', '805', '697', '822', '806', '839', '771', '732', '736', '714'], '778': ['828', '713', '705', '830', '724', '834', '695', '819', '726', '696', '734', '701', '844', '693'], '737': ['828', '705', '719', '805', '824', '745', '694', '830', '815', '728', '820', '773', '722'], '831': ['713', '705', '805', '747', '694', '853', '703', '752', '820', '773', '797', '748'], '833': ['713', '806', '768', '736'], '822': ['829', '818', '806', '839', '768', '732', '714'], '806': ['818', '833', '822', '839', '771', '732', '714'], '730': ['747', '703', '728', '849', '718', '787', '764', '687', '797', '708', '803', '783', '720', '763'], '839': ['818', '822', '806', '771', '768', '732', '714'], '771': ['688', '766', '818', '806', '839', '732', '714'], '722': ['713', '705', '719', '745', '694', '853', '784', '820', '829', '738', '711', '737', '768'], '844': ['828', '713', '705', '805', '824', '830', '697', '834', '695', '726', '848', '778', '693'], '768': ['719', '833', '822', '839', '722'], '732': ['818', '822', '806', '839', '771', '714'], '736': ['713', '724', '818', '833'], '794': ['713', '719', '745', '781', '724', '695', '784', '842', '760', '738', '701', '795'], '693': ['828', '713', '705', '824', '823', '697', '834', '695', '726', '734', '778', '844'], '763': ['705', '747', '820', '849', '787', '770', '797', '803', '741', '730'], '765': ['747', '823', '697', '703', '728', '780', '856', '687', '792', '835', '807', '779'], '714': ['818', '822', '806', '839', '771', '732']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = nx.read_gml('new_facebook_network.gml')\n",
    "neighbors = {}\n",
    "\n",
    "for node in g.nodes:\n",
    "    neighbors[node] = list(g.neighbors(node))\n",
    "\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read new_facebook_network.gml and create a fully connected graph\n",
    "\n",
    "neighbors_fully_connected = {}\n",
    "\n",
    "for node in g.nodes:\n",
    "    neighbors_fully_connected[node] = list(g.nodes)\n",
    "    neighbors_fully_connected[node].remove(node)\n",
    "\n",
    "    \n",
    "# print(neighbors_fully_connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[188, 1433], edge_index=[2, 746], y=[188])\n",
      "torch.Size([188, 1433])\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(sub_data_list[0])\n",
    "print(sub_data_list[0].x.shape)\n",
    "print(type(client_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep a  list of training and validation loss per epoch for each subgraph\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def transductive_split(data, train_percent=0.8, val_percent=0.1):\n",
    "    \"\"\"\n",
    "    Split graph data into training, validation, and testing sets for transductive learning.\n",
    "    :param data: PyG Data object\n",
    "    :param train_percent: Percentage of nodes to be used for training\n",
    "    :param val_percent: Percentage of nodes to be used for validation\n",
    "    :return: data object with train_mask, val_mask, and test_mask attributes added\n",
    "    \"\"\"\n",
    "    # set a seed for reproducibility\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    train_size = int(train_percent * num_nodes)\n",
    "    val_size = int(val_percent * num_nodes)\n",
    "\n",
    "    # Create a random permutation of node indices\n",
    "    perm = torch.randperm(num_nodes)\n",
    "\n",
    "    # Create masks for training, validation, and testing nodes\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[perm[:train_size]] = True\n",
    "    val_mask[perm[train_size:train_size + val_size]] = True\n",
    "    test_mask[perm[train_size + val_size:]] = True\n",
    "\n",
    "    # Add masks to data object\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n",
    "    \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, 32)\n",
    "        self.conv2 = GCNConv(32, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training) # p = 0.25\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAT model\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(num_features, 8, heads=8, dropout=0.6)\n",
    "        self.conv2 = GATConv(8 * 8, num_classes, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, axs = plt.subplots(10, 10, figsize=(50, 50))\n",
    "\n",
    "def train(sub_data, model, optimizer, criterion):\n",
    "      #put data on device\n",
    "      sub_data = sub_data.to(device)\n",
    "      model.train()\n",
    "      optimizer.zero_grad() \n",
    "      out = model(sub_data.x, sub_data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out, sub_data.y)  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward() \n",
    "      optimizer.step() \n",
    "      return loss\n",
    "\n",
    "def test(test_data, criterion, model):\n",
    "      model.eval()\n",
    "      out = model(test_data.x, test_data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred == test_data.y # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / len(test_data.y)  # Derive ratio of correct predictions.\n",
    "      test_loss = criterion(out, test_data.y)  # Compute validation loss\n",
    "      \n",
    "      return test_loss, test_acc\n",
    "\n",
    "\n",
    "def validate(test_data, model, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Do not compute gradients during this step\n",
    "        out = model(test_data.x, test_data.edge_index)  # Forward pass\n",
    "        pred = out.argmax(dim=1)  # Get predicted classes\n",
    "        val_correct = pred == test_data.y # Compare with ground-truth\n",
    "        val_loss = criterion(out, test_data.y)  # Compute validation loss\n",
    "        val_acc = int(val_correct.sum()) / int(len(test_data.y))  # Compute validation accuracy\n",
    "    return val_loss.item(), val_acc  # Return validation loss and accuracy\n",
    "\n",
    "\n",
    "def initial_training(epochs, learning_rate):\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    dictionary_t_1 = {}\n",
    "    dictionary_t = {}\n",
    "\n",
    "    for i in range(len(sub_data_list)):\n",
    "        sub_data = sub_data_list[i]\n",
    "        model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        train_losses = [] \n",
    "        val_losses = []    \n",
    "\n",
    "        # Training loop for each epoch (adjust the range as needed)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = train(sub_data, model, optimizer, criterion)  # Assuming 'train' returns a loss\n",
    "            val_loss, _ = validate(test_data, model, criterion)         # Assuming 'validate' returns a loss and something else (like accuracy)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "        \n",
    "        test_loss, test_acc = test(test_data, criterion, model)\n",
    "        test_losses.append(test_loss.item())\n",
    "        test_accs.append(test_acc)\n",
    "        row = i // 10\n",
    "        col = i % 10\n",
    "\n",
    "        model_weights = model.state_dict()\n",
    "        client_number_num = client_number[i]\n",
    "        dictionary_t_1[client_number_num] = model_weights\n",
    "        \n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    return dictionary_t_1, test_losses, test_accs\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_t_1, test_losses, test_accs = initial_training(10, 0.01)\n",
    "\n",
    "# average test loss and accuracy before communication \n",
    "\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(test_accs)\n",
    "plt.title(\"Test accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg(models):\n",
    "    global_model = copy.deepcopy(models[0]) # start with the first model\n",
    "    for k in global_model.keys():\n",
    "        global_model[k] = torch.stack([model[k].float() for model in models], 0).mean(0)\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_fed_avg(models, accuracies):\n",
    "    #model and accuracies are dictionaries\n",
    "    global_model = copy.deepcopy(models[0]) # start with the first model\n",
    "    for k in global_model.keys():\n",
    "        weighted_sum = 0\n",
    "        for i in range(len(models)):\n",
    "            weighted_sum += accuracies[i] * models[i][k].float()\n",
    "        global_model[k] = weighted_sum / sum(accuracies)\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_weighted_fed_avg(models, accuracies):\n",
    "    accuracies = np.exp(accuracies) / np.sum(np.exp(accuracies))\n",
    "    global_model = copy.deepcopy(models[0]) # start with the first model\n",
    "    for k in global_model.keys():\n",
    "        weighted_sum = 0\n",
    "        for i in range(len(models)):\n",
    "            weighted_sum += accuracies[i] * models[i][k].float()\n",
    "        global_model[k] = weighted_sum / sum(accuracies)\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_fed_avg(models, accuracies):\n",
    "    global_model = copy.deepcopy(models[0])\n",
    "    normalized_accuracies = [float(i)/sum(accuracies) for i in accuracies]\n",
    "    for k in global_model.keys():\n",
    "        weighted_sum = 0\n",
    "        for i in range(len(models)):\n",
    "            weighted_sum += normalized_accuracies[i] * models[i][k].float()\n",
    "        global_model[k] = weighted_sum\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_fed_avg(models, accuracies, threshold):\n",
    "    global_model = copy.deepcopy(models[0])\n",
    "    total_weight = 0\n",
    "    for k in global_model.keys():\n",
    "        weighted_sum = 0\n",
    "        unweighted_sum = 0\n",
    "        for i in range(len(models)):\n",
    "            unweighted_sum += models[i][k].float()\n",
    "            if accuracies[i] >= threshold:\n",
    "                weighted_sum += accuracies[i] * models[i][k].float()\n",
    "                total_weight += accuracies[i]\n",
    "        if total_weight == 0:\n",
    "            global_model[k] = unweighted_sum / len(models)\n",
    "        else :\n",
    "            global_model[k] = weighted_sum / total_weight\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def communication(dictionary_t_1_og, neighbors, epochs, rounds, learning_rate):\n",
    "    # make a copy of dictionary_t_1\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    dictionary_t_1 = dictionary_t_1_og.copy()\n",
    "    dictionary_t = {}\n",
    "    for k in range(1, rounds + 1):\n",
    "\n",
    "    # get the neighbprs of each client\n",
    "        for i in range(len(sub_data_list)):\n",
    "            sub_data = sub_data_list[i]\n",
    "            \n",
    "            model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)\n",
    "            model.load_state_dict(dictionary_t_1[client_number[i]])\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            train_losses = [] \n",
    "            val_losses = []    \n",
    "\n",
    "            # Training loop for each epoch (adjust the range as needed)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss = train(sub_data, model, optimizer, criterion)  # Assuming 'train' returns a loss\n",
    "                val_loss, _ = validate(test_data, model, criterion)         # Assuming 'validate' returns a loss and something else (like accuracy)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            \n",
    "            test_loss, test_acc = test(test_data, criterion, model)\n",
    "            if k == rounds:\n",
    "                test_losses.append(test_loss.item())\n",
    "                test_accs.append(test_acc)\n",
    "\n",
    "        \n",
    "            # add the model weights to the t -1 dictionary\n",
    "            model_weights = model.state_dict()\n",
    "            client_number_num = client_number[i]\n",
    "            dictionary_t_1[client_number_num] = model_weights\n",
    "        \n",
    "\n",
    "        for i in range(100):\n",
    "            client_number_num = client_number[i]\n",
    "            # go through neighbors of client_number\n",
    "            string_client_num = str(client_number_num)\n",
    "            client_neighbors = neighbors[string_client_num]\n",
    "            \n",
    "            neighbors_state_dicts = []\n",
    "            neighbors_state_dicts.append(dictionary_t_1[client_number_num])\n",
    "            for j in range(len(client_neighbors)):\n",
    "                # get model weights of neighbor\n",
    "                neighbor_model = dictionary_t_1[int(client_neighbors[j])]\n",
    "                neighbors_state_dicts.append(neighbor_model)\n",
    "            \n",
    "            #call fed_avg\n",
    "            average_state_dict = {}\n",
    "            average_state_dict = fed_avg(neighbors_state_dicts)\n",
    "                \n",
    "                \n",
    "            dictionary_t[client_number_num] = average_state_dict    \n",
    "            \n",
    "        dictionary_t_1 = dictionary_t\n",
    "\n",
    "    return test_losses, test_accs\n",
    " \n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#communication with weight averaging based on model accuracy\n",
    "\n",
    "\n",
    "def communication(dictionary_t_1_og, neighbors, epochs, rounds, learning_rate):\n",
    "    # make a copy of dictionary_t_1\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    dictionary_t_1 = dictionary_t_1_og.copy()\n",
    "    t_1_accuracies = {}\n",
    "    dictionary_t = {}\n",
    "    for k in range(1, rounds + 1):\n",
    "\n",
    "    # get the neighbprs of each client\n",
    "        for i in range(len(sub_data_list)):\n",
    "            sub_data = sub_data_list[i]\n",
    "            \n",
    "            model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)\n",
    "            model.load_state_dict(dictionary_t_1[client_number[i]])\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            train_losses = [] \n",
    "            val_losses = []    \n",
    "\n",
    "            # Training loop for each epoch (adjust the range as needed)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss = train(sub_data, model, optimizer, criterion)  # Assuming 'train' returns a loss\n",
    "                val_loss, _ = validate(test_data, model, criterion)         # Assuming 'validate' returns a loss and something else (like accuracy)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            \n",
    "            test_loss, test_acc = test(test_data, criterion, model)\n",
    "            if k == rounds:\n",
    "                test_losses.append(test_loss.item())\n",
    "                test_accs.append(test_acc)\n",
    "\n",
    "        \n",
    "            # add the model weights to the t -1 dictionary\n",
    "            model_weights = model.state_dict()\n",
    "            client_number_num = client_number[i]\n",
    "            dictionary_t_1[client_number_num] = model_weights\n",
    "            t_1_accuracies[client_number_num] = test_acc\n",
    "        \n",
    "\n",
    "        for i in range(100):\n",
    "            client_number_num = client_number[i]\n",
    "            # go through neighbors of client_number\n",
    "            string_client_num = str(client_number_num)\n",
    "            client_neighbors = neighbors[string_client_num]\n",
    "            \n",
    "            neighbors_state_dicts = []\n",
    "            neighbors_state_dicts.append(dictionary_t_1[client_number_num])\n",
    "            accuracies = []\n",
    "            accuracies.append(t_1_accuracies[client_number_num])\n",
    "            for j in range(len(client_neighbors)):\n",
    "                # get model weights of neighbor\n",
    "                neighbor_model = dictionary_t_1[int(client_neighbors[j])]\n",
    "                neighbors_state_dicts.append(neighbor_model)\n",
    "                accuracies.append(t_1_accuracies[int(client_neighbors[j])])\n",
    "                \n",
    "            \n",
    "            #call fed_avg\n",
    "            average_state_dict = {}\n",
    "            average_state_dict = threshold_fed_avg(neighbors_state_dicts, accuracies, 0.15)\n",
    "                \n",
    "                \n",
    "            dictionary_t[client_number_num] = average_state_dict    \n",
    "            \n",
    "        dictionary_t_1 = dictionary_t\n",
    "\n",
    "    return test_losses, test_accs\n",
    " \n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 10, 10, 0.01)\n",
    "\n",
    "# average test loss and accuracy after communication\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "\n",
    "#mean of test_accs\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a distriution of the accuracies of test_losses and test_accs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(test_accs)\n",
    "plt.title(\"Test accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 epoch, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(1)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors_fully_connected, 1, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(5)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors_fully_connected, 5, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(10)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors_fully_connected, 10, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Social Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 epoch, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(1)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 1, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(5, 0.01)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 5, 100, 0.01)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the test accuracies\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(test_accs)\n",
    "plt.title(\"Test accuracies\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(10)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 10, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(20)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 20, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 epochs, 10 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(1)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 1, 10)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 epochs, 10 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(5)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 5, 10)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 epochs, 10 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(10)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 10, 10)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Graph Results\n",
    "### (Training with original subgraphs)\n",
    "\n",
    "| Epochs | Communication Rounds | Avg Test Accuracy (Before) | Avg Test Accuracy (After) | Avg Test Loss (Before) | Avg Test Loss (After) |\n",
    "| ------ | -------------------- | -------------------------- | ------------------------- | ---------------------- | --------------------- |\n",
    "| 1      | 100                  | 15.07%                     | 14.29%                    | 1.947                  | 2.083                 |\n",
    "| 5      | 100                  | 14.89%                     | 15.49%                    | 2.021                  | 2.943                 |\n",
    "| 10     | 100                  | 14.73%                     | 14.90%                    | 2.387                  | 5.010                 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Network Graph Results\n",
    "### (Training with original subgraphs)\n",
    "\n",
    "| Epochs | Communication Rounds | Avg Test Accuracy (Before) | Avg Test Accuracy (After) | Avg Test Loss (Before) | Avg Test Loss (After) |\n",
    "| ------ | -------------------- | -------------------------- | ------------------------- | ---------------------- | --------------------- |\n",
    "| 1      | 100                  | 15.13%                     | 14.31%                    | 1.947                  | 1.996                 |\n",
    "| 1      | 10                   | 15.40%                     | 15.12%                    | 1.946                  | 1.945                 |\n",
    "| 5      | 100                  | 14.38%                     | 16.16%                    | 2.028                  | 2.730                 |\n",
    "| 5      | 10                   | 14.52%                     | 14.49%                    | 2.014                  | 2.282                 |\n",
    "| 10     | 100                  | 14.74%                     | 16.07%                    | 2.321                  | 4.402                 |\n",
    "| 10     | 10                   | 14.69%                     | 15.01%                    | 2.349                  | 3.084                 |\n",
    "| 20     | 100                  | 15.39%                     | 18.50%                    | 3.101                  | 4.654                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Graph Results\n",
    "### (Training with second order subgraphs)\n",
    "\n",
    "\n",
    "| Epochs | Communication Rounds | Avg Test Accuracy (Before) | Avg Test Accuracy (After) | Avg Test Loss (Before) | Avg Test Loss (After) |\n",
    "| ------ | -------------------- | -------------------------- | ------------------------- | ---------------------- | --------------------- |\n",
    "| 1      | 100                  | 15.02%                     | 19.24%                    | 1.951                  | 2.052                 |\n",
    "| 5      | 100                  | 15.22%                     | 31.80%                    | 2.114                 | 2.614                |\n",
    "| 10     | 100                  | 15.69%                     | 29.41%                    | 2.794                 | 2.932              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Network Graphs Results\n",
    "### (Training with second order subgraphs)\n",
    "\n",
    "| Epochs | Communication Rounds | Avg Test Accuracy (Before) | Avg Test Accuracy (After) | Avg Test Loss (Before) | Avg Test Loss (After) |\n",
    "|--------|----------------------|----------------------------|---------------------------|-----------------------|-----------------------|\n",
    "| 1      | 100                  | 0.1532                     | 0.1961                    | 1.9492                | 1.9300                |\n",
    "| 5      | 100                  | 0.1523                     | 0.3180                    | 2.1628                | 2.2851                |\n",
    "| 10     | 100                  | 0.1593                     | 0.3135                    | 2.7872                | 2.9388                |\n",
    "| 20     | 100                  | 0.1779                     | 0.2935                    | 3.4153                | 3.4956                |\n",
    "| 1      | 10                   | 0.1486                     | 0.1543                    | 1.9489                | 1.9431                |\n",
    "| 5      | 10                   | 0.1514                     | 0.1521                    | 2.1468                | 2.3963                |\n",
    "| 10     | 10                   | 0.1590                     | 0.1783                    | 2.6969                | 3.1621                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized Training on Entire Cora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Dataset class\n",
    "\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, graph_data):\n",
    "        self.graph_data = graph_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.graph_data.x[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        print(idx)\n",
    "        x = self.graph_data.x[idx,:]\n",
    "        y = self.graph_data.y[idx]\n",
    "        edge_index = self.graph_data.edge_index\n",
    "        # connected_edges = (self.graph_data.edge_index[0] == idx) | (self.graph_data.edge_index[1] == idx)\n",
    "        # edge_index = self.graph_data.edge_index[:, connected_edges]\n",
    "        return x, edge_index, y, idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cora_graph_training \n",
    "\n",
    "import os\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "cora_graph_training = nx.read_gml('cora_graph_training.gml')\n",
    "test_graph = test_data\n",
    "\n",
    "\n",
    "graph_nodes = list(cora_graph_training.nodes)\n",
    "graph_nodes = [int(node) for node in graph_nodes]  # Convert to integer if they are not\n",
    "\n",
    "graph_edge_index, _ = subgraph(graph_nodes, data.edge_index, relabel_nodes=True)\n",
    "graph_data = Data(x=data.x[graph_nodes], edge_index=graph_edge_index, y=data.y[graph_nodes])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n",
    "\n",
    "def train_test(graph_data, epochs, dataset, test_data, device):\n",
    "    # model = GCN(graph_data.num_node_features, dataset.num_classes).to(device)\n",
    "    model  = GAT(graph_data.num_node_features, dataset.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()  \n",
    "    epochs = 10\n",
    "    # Training loop for each epoch (adjust the range as needed)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(graph_data, model, optimizer, criterion)  # Assuming 'train' returns a loss\n",
    "        val_loss, _ = validate(test_data, model, criterion)         # Assuming 'validate' returns a loss and something else (like accuracy) \n",
    "\n",
    "    test_loss, test_acc = test(test_data, criterion, model)\n",
    "    \n",
    "    # print(\"Centralized learning test loss: \", test_loss)\n",
    "    print(\"Centralized learning test accuracy: \", test_acc)\n",
    "\n",
    "train_test(graph_data, 10, dataset, test_data, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized Learning with Different Batch Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "def train_test_batches(graph_data, epochs, dataset, test_data, device, batch_size):\n",
    "    model = GCN(graph_data.num_node_features, dataset.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()  \n",
    "    epochs = 10\n",
    "    # Training loop for each epoch (adjust the range as needed)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_batches(graph_data, model, optimizer, criterion, batch_size)  # Assuming 'train' returns a loss\n",
    "        val_loss, _ = validate(test_data, model, criterion)         # Assuming 'validate' returns a loss and something else (like accuracy) \n",
    "\n",
    "    test_loss, test_acc = test(test_data, criterion, model)\n",
    "    \n",
    "    # print(\"Centralized learning test loss: \", test_loss)\n",
    "    print(\"Batch size: \", batch_size)\n",
    "    print(\"Centralized learning test accuracy: \", test_acc)\n",
    "\n",
    "def train_batches(graph_data, model, optimizer, criterion, batch_size):\n",
    "    # graph_dataset = [graph_data.x, graph_data.edge_index, graph_data.y]\n",
    "    # print(graph_data.edge_index.shape)\n",
    "    graph_dataset = GraphDataset(graph_data)\n",
    "    graph_data_loader = DataLoader(graph_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # print(len(graph_data_loader.dataset))\n",
    "    \n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(graph_data_loader):\n",
    "      x, edge_index, y, idx = data\n",
    "    #   print(idx)\n",
    "      nodes = list(idx)\n",
    "      sub_edge_index, _ = subgraph(nodes, edge_index[0], relabel_nodes=True)\n",
    "    #   print(sub_edge_index.shape)\n",
    "      optimizer.zero_grad() \n",
    "      out = model(x, sub_edge_index) \n",
    "      loss = criterion(out, y)  \n",
    "      loss.backward() \n",
    "      optimizer.step() \n",
    "      total_loss += loss.item()\n",
    "      \n",
    "    return total_loss/(i+1)\n",
    "\n",
    "print()\n",
    "batch_sizes = [10, 16, 32]\n",
    "for batch_size in batch_sizes:\n",
    "    train_test_batches(graph_data, 10, dataset, test_data, device, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our Social Network\n",
    "\n",
    "### Adam Optimizer, changing learning rates\n",
    "### 1 epoch, 100 communication rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    dictionary_t_1, test_losses, test_accs = initial_training(1, learning_rate)\n",
    "\n",
    "    test_losses, test_accs = communication(dictionary_t_1, neighbors, 1, 100, learning_rate)\n",
    "    mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "    mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "    print(\"learning rate: \", learning_rate)\n",
    "    print(\"mean test accuracy after communication: \", mean_test_accs)\n",
    "    print(\"mean test loss after communication: \", mean_test_losses)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    dictionary_t_1, test_losses, test_accs = initial_training(5, learning_rate)\n",
    "\n",
    "    test_losses, test_accs = communication(dictionary_t_1, neighbors, 5, 100, learning_rate)\n",
    "    mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "    mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "    print(\"learning rate: \", learning_rate)\n",
    "    print(\"mean test accuracy after communication: \", mean_test_accs)\n",
    "    print(\"mean test loss after communication: \", mean_test_losses) # print(f\"{mean_test_losses:.4f},{}\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in learning_rates:\n",
    "    dictionary_t_1, test_losses, test_accs = initial_training(10, learning_rate)\n",
    "\n",
    "    test_losses, test_accs = communication(dictionary_t_1, neighbors, 10, 100, learning_rate)\n",
    "    mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "    mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "    print(\"learning rate: \", learning_rate)\n",
    "    print(\"mean test accuracy after communication: \", mean_test_accs)\n",
    "    print(\"mean test loss after communication: \", mean_test_losses)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_nodes = 1000000\n",
    "for subgraph in sub_data_list:\n",
    "    if subgraph.num_nodes < min_nodes:\n",
    "        min_nodes = subgraph.num_nodes\n",
    "print(\"min nodes: \", min_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initial_training(epochs, learning_rate, batch_size):\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    dictionary_t_1 = {}\n",
    "    dictionary_t = {}\n",
    "    \n",
    "    for i in range(len(sub_data_list)):\n",
    "        sub_data = sub_data_list[i]\n",
    "        model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        train_losses = [] \n",
    "        val_losses = []    \n",
    "\n",
    "        # Training loop for each epoch (adjust the range as needed)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = train_batches(sub_data, model, optimizer, criterion, batch_size)  # Assuming 'train' returns a loss\n",
    "            val_loss, _ = validate(test_data, model, criterion)         # Assuming 'validate' returns a loss and something else (like accuracy)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "        \n",
    "        test_loss, test_acc = test(test_data, criterion, model)\n",
    "        test_losses.append(test_loss.item())\n",
    "        test_accs.append(test_acc)\n",
    "        row = i // 10\n",
    "        col = i % 10\n",
    "\n",
    "        model_weights = model.state_dict()\n",
    "        client_number_num = client_number[i]\n",
    "        dictionary_t_1[client_number_num] = model_weights\n",
    "    return dictionary_t_1, test_losses, test_accs\n",
    "        \n",
    "def communication(dictionary_t_1_og, neighbors, epochs, rounds, learning_rate, batch_size):\n",
    "    # make a copy of dictionary_t_1\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    dictionary_t_1 = dictionary_t_1_og.copy()\n",
    "    dictionary_t = {}\n",
    "    for k in range(1, rounds + 1):\n",
    "\n",
    "    # get the neighbprs of each client\n",
    "        for i in range(len(sub_data_list)):\n",
    "            sub_data = sub_data_list[i]\n",
    "            \n",
    "            model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)\n",
    "            model.load_state_dict(dictionary_t_1[client_number[i]])\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            train_losses = [] \n",
    "            val_losses = []    \n",
    "\n",
    "            # Training loop for each epoch (adjust the range as needed)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss = train_batches(sub_data, model, optimizer, criterion, batch_size)  # Assuming 'train' returns a loss\n",
    "                val_loss, _ = validate(test_data, model, criterion)         # Assuming 'validate' returns a loss and something else (like accuracy)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)                   \n",
    "            test_loss, test_acc = test(test_data, criterion, model)\n",
    "            if k == rounds:\n",
    "                test_losses.append(test_loss.item())\n",
    "                test_accs.append(test_acc)\n",
    "            # add the model weights to the t -1 dictionary\n",
    "            model_weights = model.state_dict()\n",
    "            client_number_num = client_number[i]\n",
    "            dictionary_t_1[client_number_num] = model_weights\n",
    "        \n",
    "\n",
    "        for i in range(100):\n",
    "            client_number_num = client_number[i]\n",
    "            # go through neighbors of client_number\n",
    "            string_client_num = str(client_number_num)\n",
    "            client_neighbors = neighbors[string_client_num]\n",
    "            \n",
    "            neighbors_state_dicts = []\n",
    "            neighbors_state_dicts.append(dictionary_t_1[client_number_num])\n",
    "            for j in range(len(client_neighbors)):\n",
    "                # get model weights of neighbor\n",
    "                neighbor_model = dictionary_t_1[int(client_neighbors[j])]\n",
    "                neighbors_state_dicts.append(neighbor_model)\n",
    "            \n",
    "            #call fed_avg\n",
    "            average_state_dict = {}\n",
    "            average_state_dict = fed_avg(neighbors_state_dicts)\n",
    "            dictionary_t[client_number_num] = average_state_dict    \n",
    "            \n",
    "        dictionary_t_1 = dictionary_t\n",
    "    return test_losses, test_accs\n",
    "    \n",
    "    \n",
    "\n",
    "def train_batches(graph_data, model, optimizer, criterion, batch_size):\n",
    "    # graph_dataset = [graph_data.x, graph_data.edge_index, graph_data.y]\n",
    "    graph_dataset = GraphDataset(graph_data)\n",
    "    graph_data_loader = DataLoader(graph_dataset, batch_size=batch_size, shuffle=True)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(graph_data_loader):\n",
    "      x, edge_index, y, idx = data\n",
    "      nodes = list(idx)\n",
    "      sub_edge_index, _ = subgraph(nodes, edge_index[0], relabel_nodes=True)\n",
    "    #   print(sub_edge_index.shape)\n",
    "      optimizer.zero_grad() \n",
    "      out = model(x, sub_edge_index) \n",
    "      loss = criterion(out, y)  \n",
    "      loss.backward() \n",
    "      optimizer.step() \n",
    "      total_loss += loss.item()  \n",
    "    return total_loss/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 188 nodes\n",
    "\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, graph_data):\n",
    "        \n",
    "        self.graph_data = graph_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.graph_data.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = self.graph_data.x[idx,:]\n",
    "        y = self.graph_data.y[idx]\n",
    "        edge_index = self.graph_data.edge_index\n",
    "        # connected_edges = (self.graph_data.edge_index[0] == idx) | (self.graph_data.edge_index[1] == idx)\n",
    "        # edge_index = self.graph_data.edge_index[:, connected_edges]\n",
    "        return x, edge_index, y, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Adam Optimizer, lr = 0.01, and varying batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [21, 16, 32, 64]\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    dictionary_t_1, test_losses, test_accs = initial_training(1, 0.01 , batch_size)\n",
    "\n",
    "    test_losses, test_accs = communication(dictionary_t_1, neighbors, 1, 100, 0.01, batch_size)\n",
    "    mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "    mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "    print(\"batch size: \", batch_size)\n",
    "    print(\"mean test accuracy after communication: \", mean_test_accs)\n",
    "    print(\"mean test loss after communication: \", mean_test_losses)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 100, 128]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    dictionary_t_1, test_losses, test_accs = initial_training(5, 0.01 , batch_size)\n",
    "\n",
    "    test_losses, test_accs = communication(dictionary_t_1, neighbors, 5, 100, 0.01, batch_size)\n",
    "    mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "    mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "    print(\"batch size: \", batch_size)\n",
    "    print(\"mean test accuracy after communication: \", mean_test_accs)\n",
    "    print(\"mean test loss after communication: \", mean_test_losses)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [16, 32, 64, 100, 128]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    dictionary_t_1, test_losses, test_accs = initial_training(10, 0.01 , batch_size)\n",
    "\n",
    "    test_losses, test_accs = communication(dictionary_t_1, neighbors, 10, 100, 0.01, batch_size)\n",
    "    mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "    mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "    print(\"batch size: \", batch_size)\n",
    "    print(\"mean test accuracy after communication: \", mean_test_accs)\n",
    "    print(\"mean test loss after communication: \", mean_test_losses)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Federated Averaging Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized federated averaging technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_t_1, test_losses, test_accs = initial_training(5, 0.01)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 5, 100, 0.01)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential weighted federated averaging technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(5, 0.01)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 5, 100, 0.01)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold federated averaging technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_t_1, test_losses, test_accs = initial_training(5, 0.01)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 5, 100, 0.01)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Privacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DifferentiallyPrivateGCN(nn.Module):\n",
    "    def __init__(self, gcn_model, lr, noise_multiplier):\n",
    "        super(DifferentiallyPrivateGCN, self).__init__()\n",
    "        self.gcn_model = gcn_model\n",
    "        self.lr = lr\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "        self.optimizer = torch.optim.Adam(self.gcn_model.parameters(), lr=self.lr, weight_decay=5e-4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.gcn_model(x, edge_index)\n",
    "\n",
    "    def train_step(self, x, edge_index, y):\n",
    "        # self.gcn_model.zero_grad()\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.forward(x, edge_index)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, y)\n",
    "        # loss = F.nll_loss(output, y)\n",
    "        loss.backward() \n",
    "        \n",
    "        for param in self.gcn_model.parameters():\n",
    "            noise = torch.randn_like(param.grad) * self.noise_multiplier\n",
    "            param.grad.add_(noise)\n",
    "            # param.data.add_(-self.lr * param.grad)\n",
    "            \n",
    "        self.optimizer.step()\n",
    "\n",
    "        return output, loss\n",
    "\n",
    "    #   sub_data = sub_data.to(device)\n",
    "    #   model.train()\n",
    "    #   optimizer.zero_grad() \n",
    "    #   out = model(sub_data.x, sub_data.edge_index)  # Perform a single forward pass.\n",
    "    #   loss = criterion(out, sub_data.y)  # Compute the loss solely based on the training nodes.\n",
    "    #   loss.backward() \n",
    "    #   optimizer.step() \n",
    "    \n",
    "    # def load_state_dict(self, state_dict):\n",
    "    #     state_dict = {k.replace('gcn_model.', ''): v for k, v in state_dict.items()}\n",
    "    #     self.gcn_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "\n",
    "def test(test_data, criterion, model):\n",
    "      model.eval()\n",
    "      out = model(test_data.x, test_data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred == test_data.y # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / len(test_data.y)  # Derive ratio of correct predictions.\n",
    "      test_loss = criterion(out, test_data.y)  # Compute validation loss\n",
    "      \n",
    "      return test_loss, test_acc\n",
    "\n",
    "\n",
    "def validate(test_data, model, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Do not compute gradients during this step\n",
    "        out = model(test_data.x, test_data.edge_index)  # Forward pass\n",
    "        pred = out.argmax(dim=1)  # Get predicted classes\n",
    "        val_correct = pred == test_data.y # Compare with ground-truth\n",
    "        val_loss = criterion(out, test_data.y)  # Compute validation loss\n",
    "        val_acc = int(val_correct.sum()) / int(len(test_data.y))  # Compute validation accuracy\n",
    "    return val_loss.item(), val_acc  \n",
    "\n",
    "\n",
    "def initial_training(epochs, learning_rate):\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    dictionary_t_1 = {}\n",
    "    dictionary_t = {}\n",
    "\n",
    "    for i in range(len(sub_data_list)):\n",
    "        sub_data = sub_data_list[i]\n",
    "        gcn_model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)\n",
    "        model = DifferentiallyPrivateGCN(gcn_model, learning_rate, 0.01)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        train_losses = [] \n",
    "        val_losses = []    \n",
    "\n",
    "        # Training loop for each epoch (adjust the range as needed)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()  # Set the model to training mode\n",
    "            output, loss = model.train_step(sub_data.x, sub_data.edge_index, sub_data.y)\n",
    "            train_losses.append(loss.item())\n",
    "            val_loss, _ = validate(test_data, model, criterion)  # Assuming 'validate' returns a loss and something else (like accuracy)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "        \n",
    "        test_loss, test_acc = test(test_data, criterion, model)\n",
    "        test_losses.append(test_loss.item())\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "\n",
    "        model_weights =  model.gcn_model.state_dict()\n",
    "        client_number_num = client_number[i]\n",
    "        dictionary_t_1[client_number_num] = model_weights\n",
    "        \n",
    "    \n",
    "    return dictionary_t_1, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def communication(dictionary_t_1_og, neighbors, epochs, rounds, learning_rate):\n",
    "    # make a copy of dictionary_t_1\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    dictionary_t_1 = dictionary_t_1_og.copy()\n",
    "    dictionary_t = {}\n",
    "    for k in range(1, rounds + 1):\n",
    "\n",
    "    # get the neighbprs of each client\n",
    "        for i in range(len(sub_data_list)):\n",
    "            sub_data = sub_data_list[i]\n",
    "            \n",
    "            gcn_model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)\n",
    "            state_dict = dictionary_t_1[client_number[i]]\n",
    "            gcn_model.load_state_dict(state_dict)\n",
    "            model = DifferentiallyPrivateGCN(gcn_model, learning_rate, 0.01)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            train_losses = [] \n",
    "            val_losses = []    \n",
    "\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                \n",
    "                model.train()\n",
    "                output, loss = model.train_step(sub_data.x, sub_data.edge_index, sub_data.y)\n",
    "                train_losses.append(loss.item())\n",
    "                val_loss, _ = validate(test_data, model, criterion)\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            \n",
    "            test_loss, test_acc = test(test_data, criterion, model)\n",
    "            if k == rounds:\n",
    "                test_losses.append(test_loss.item())\n",
    "                test_accs.append(test_acc)\n",
    "\n",
    "        \n",
    "            # add the model weights to the t -1 dictionary\n",
    "            model_weights =  model.gcn_model.state_dict()\n",
    "            client_number_num = client_number[i]\n",
    "            dictionary_t_1[client_number_num] = model_weights\n",
    "        \n",
    "\n",
    "        for i in range(100):\n",
    "            client_number_num = client_number[i]\n",
    "            # go through neighbors of client_number\n",
    "            string_client_num = str(client_number_num)\n",
    "            client_neighbors = neighbors[string_client_num]\n",
    "            \n",
    "            neighbors_state_dicts = []\n",
    "            neighbors_state_dicts.append(dictionary_t_1[client_number_num])\n",
    "            for j in range(len(client_neighbors)):\n",
    "                # get model weights of neighbor\n",
    "                neighbor_model = dictionary_t_1[int(client_neighbors[j])]\n",
    "                neighbors_state_dicts.append(neighbor_model)\n",
    "            \n",
    "            #call fed_avg\n",
    "            average_state_dict = {}\n",
    "            average_state_dict = fed_avg(neighbors_state_dicts)\n",
    "                \n",
    "            dictionary_t[client_number_num] = average_state_dict    \n",
    "            \n",
    "        dictionary_t_1 = dictionary_t\n",
    "\n",
    "    return test_losses, test_accs\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test accuracy after communication:  0.6669329073482426\n",
      "mean test loss after communication:  1.173405447602272\n"
     ]
    }
   ],
   "source": [
    "dictionary_t_1, test_losses, test_accs = initial_training(5, 0.01)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 5, 100, 0.01)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
