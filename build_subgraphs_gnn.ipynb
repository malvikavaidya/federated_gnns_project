{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234567)\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "folder_path = 'client_subgraphs'\n",
    "sub_data_list = []\n",
    "client_number = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.gml'):\n",
    "      \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        g = nx.read_gml(file_path)\n",
    "\n",
    "        subgraph_nodes = list(g.nodes)\n",
    "        subgraph_nodes = [int(node) for node in subgraph_nodes]  # Convert to integer if they are not\n",
    "\n",
    "        sub_edge_index, _ = subgraph(subgraph_nodes, data.edge_index, relabel_nodes=True)\n",
    "\n",
    "        sub_data = Data(x=data.x[subgraph_nodes], edge_index=sub_edge_index, y=data.y[subgraph_nodes])\n",
    "        sub_data_list.append(sub_data)\n",
    "        client_number.append(int(filename.split('.')[0].split('_')[1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = None\n",
    "\n",
    "g = nx.read_gml('test_graph.gml')\n",
    "subgraph_nodes = list(g.nodes)\n",
    "subgraph_nodes = [int(node) for node in subgraph_nodes]  # Convert to integer if they are not\n",
    "sub_edge_index, _ = subgraph(subgraph_nodes, data.edge_index, relabel_nodes=True)\n",
    "test_data = Data(x=data.x[subgraph_nodes], edge_index=sub_edge_index, y=data.y[subgraph_nodes])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'828': ['713', '705', '719', '805', '824', '745', '747', '823', '694', '830', '781', '697', '724', '827', '688', '853', '834', '703', '784', '815', '752', '728', '820', '842', '800', '819', '774', '726', '773', '766', '829', '780', '810', '696', '849', '718', '754', '760', '739', '856', '764', '687', '840', '847', '770', '797', '814', '727', '731', '708', '698', '817', '792', '845', '838', '848', '835', '807', '711', '779', '706', '772', '741', '709', '778', '737', '844', '693'], '713': ['828', '705', '719', '805', '824', '745', '747', '823', '694', '830', '781', '697', '724', '827', '688', '853', '834', '695', '784', '815', '752', '842', '800', '819', '774', '726', '773', '766', '829', '780', '810', '696', '760', '787', '840', '847', '814', '731', '708', '698', '738', '817', '734', '845', '701', '848', '748', '706', '777', '772', '818', '778', '831', '833', '722', '844', '736', '794', '693'], '705': ['828', '713', '719', '805', '824', '745', '747', '823', '694', '830', '781', '697', '724', '827', '853', '834', '703', '784', '752', '728', '820', '842', '800', '819', '726', '773', '829', '780', '810', '849', '718', '754', '760', '739', '787', '764', '847', '770', '814', '731', '738', '845', '835', '807', '758', '803', '748', '755', '809', '720', '706', '772', '741', '778', '737', '831', '722', '844', '693', '763'], '719': ['828', '713', '705', '805', '824', '745', '747', '823', '694', '830', '781', '697', '724', '827', '688', '853', '834', '703', '695', '784', '815', '752', '728', '820', '842', '800', '819', '774', '726', '773', '766', '829', '810', '696', '849', '718', '760', '787', '764', '840', '847', '814', '727', '731', '708', '698', '817', '734', '845', '848', '755', '720', '706', '772', '737', '722', '768', '794'], '805': ['828', '713', '705', '719', '824', '745', '747', '823', '694', '781', '697', '724', '827', '688', '853', '834', '703', '695', '815', '752', '728', '842', '800', '819', '774', '726', '766', '829', '780', '810', '696', '718', '754', '760', '739', '764', '687', '840', '847', '814', '727', '731', '738', '817', '792', '734', '838', '807', '706', '772', '709', '818', '737', '831', '844'], '824': ['828', '713', '705', '719', '805', '745', '823', '694', '830', '781', '697', '724', '827', '688', '853', '834', '703', '695', '784', '815', '752', '728', '820', '842', '819', '774', '726', '773', '766', '829', '780', '696', '849', '754', '760', '739', '764', '840', '847', '770', '727', '708', '792', '838', '848', '835', '779', '777', '709', '737', '844', '693'], '745': ['828', '713', '705', '719', '805', '824', '747', '823', '694', '830', '781', '697', '724', '827', '853', '834', '695', '784', '815', '752', '820', '842', '800', '774', '726', '773', '829', '810', '696', '718', '760', '787', '847', '814', '731', '698', '738', '817', '845', '758', '711', '755', '720', '706', '795', '737', '722', '794'], '747': ['828', '713', '705', '719', '805', '745', '823', '830', '697', '688', '703', '815', '752', '728', '820', '800', '819', '774', '773', '780', '810', '696', '849', '718', '754', '739', '856', '764', '687', '840', '770', '797', '708', '698', '792', '838', '701', '848', '807', '793', '758', '803', '748', '783', '809', '741', '831', '730', '763', '765'], '823': ['828', '713', '705', '719', '805', '824', '745', '747', '694', '830', '781', '697', '724', '827', '688', '853', '834', '703', '815', '752', '728', '820', '800', '819', '774', '726', '773', '766', '780', '810', '696', '718', '754', '760', '739', '787', '856', '764', '687', '840', '708', '698', '792', '838', '848', '835', '807', '779', '741', '709', '693', '765'], '694': ['828', '713', '705', '719', '805', '824', '745', '823', '830', '781', '697', '724', '827', '688', '853', '834', '703', '695', '784', '815', '752', '820', '842', '800', '774', '726', '773', '766', '829', '780', '810', '696', '754', '760', '847', '727', '731', '738', '845', '848', '711', '706', '795', '772', '737', '831', '722'], '830': ['828', '713', '705', '719', '824', '745', '747', '823', '694', '781', '697', '724', '827', '688', '853', '834', '695', '815', '752', '820', '800', '819', '774', '726', '766', '780', '696', '849', '754', '760', '856', '687', '840', '847', '727', '731', '817', '792', '845', '838', '701', '748', '777', '709', '778', '737', '844'], '781': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '697', '724', '827', '853', '834', '703', '695', '784', '815', '752', '728', '842', '800', '726', '773', '829', '810', '849', '718', '760', '787', '764', '770', '814', '731', '708', '738', '817', '701', '793', '758', '755', '783', '809', '720', '772', '741', '794'], '697': ['828', '713', '705', '719', '805', '824', '745', '747', '823', '694', '830', '781', '724', '827', '688', '834', '695', '815', '820', '842', '800', '819', '726', '766', '780', '696', '849', '754', '760', '856', '687', '840', '727', '731', '708', '698', '701', '848', '835', '748', '777', '772', '709', '818', '844', '693', '765'], '724': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '697', '827', '688', '853', '834', '695', '784', '752', '820', '842', '819', '726', '773', '766', '829', '696', '754', '760', '814', '731', '708', '738', '734', '701', '848', '711', '795', '777', '778', '736', '794'], '827': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '697', '724', '853', '834', '703', '784', '752', '842', '800', '819', '773', '766', '810', '849', '718', '760', '787', '764', '797', '814', '731', '708', '817', '758', '755', '783', '809', '772', '741'], '688': ['828', '713', '719', '805', '824', '747', '823', '694', '830', '697', '724', '853', '834', '695', '815', '752', '800', '819', '774', '726', '766', '780', '696', '849', '754', '739', '856', '687', '840', '847', '770', '727', '792', '734', '845', '701', '777', '772', '771'], '853': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '724', '827', '688', '834', '695', '784', '815', '752', '820', '842', '800', '774', '726', '773', '766', '829', '810', '696', '847', '814', '727', '731', '738', '734', '711', '706', '795', '831', '722'], '834': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '697', '724', '827', '688', '853', '695', '784', '815', '752', '820', '842', '800', '819', '726', '773', '766', '829', '780', '810', '696', '718', '760', '847', '814', '731', '701', '706', '778', '844', '693'], '703': ['828', '705', '719', '805', '824', '747', '823', '694', '781', '827', '815', '728', '820', '819', '718', '754', '739', '787', '856', '764', '687', '797', '698', '817', '792', '845', '838', '835', '807', '793', '758', '748', '779', '809', '741', '709', '831', '730', '765'], '695': ['713', '719', '805', '824', '745', '694', '830', '781', '697', '724', '688', '853', '834', '784', '815', '752', '842', '726', '766', '829', '780', '810', '696', '718', '754', '760', '840', '847', '738', '734', '701', '711', '706', '777', '778', '844', '794', '693'], '784': ['828', '713', '705', '719', '824', '745', '694', '781', '724', '827', '853', '834', '695', '752', '842', '800', '774', '726', '773', '766', '829', '810', '696', '760', '727', '731', '738', '734', '711', '706', '795', '722', '794'], '815': ['828', '713', '719', '805', '824', '745', '747', '823', '694', '830', '781', '697', '688', '853', '834', '703', '695', '752', '819', '726', '773', '766', '780', '810', '718', '754', '856', '847', '731', '792', '845', '838', '835', '706', '737'], '752': ['828', '713', '705', '719', '805', '824', '745', '747', '823', '694', '830', '781', '724', '827', '688', '853', '834', '695', '784', '815', '820', '842', '800', '774', '726', '773', '766', '829', '810', '847', '727', '731', '738', '711', '706', '831'], '728': ['828', '705', '719', '805', '824', '747', '823', '781', '703', '820', '819', '774', '773', '780', '754', '739', '787', '856', '764', '687', '797', '817', '792', '838', '835', '807', '803', '783', '779', '809', '741', '737', '730', '765'], '820': ['828', '705', '719', '824', '745', '747', '823', '694', '830', '697', '724', '853', '834', '703', '752', '728', '774', '726', '773', '766', '849', '760', '687', '770', '797', '845', '848', '803', '783', '795', '741', '737', '831', '722', '763'], '842': ['828', '713', '705', '719', '805', '824', '745', '694', '781', '697', '724', '827', '853', '834', '695', '784', '752', '800', '773', '829', '810', '696', '760', '814', '727', '738', '734', '701', '711', '795', '794'], '800': ['828', '713', '705', '719', '805', '745', '747', '823', '694', '830', '781', '697', '827', '688', '853', '834', '784', '752', '842', '774', '766', '810', '760', '856', '814', '727', '708', '698', '845', '772', '741'], '819': ['828', '713', '705', '719', '805', '824', '747', '823', '830', '697', '724', '827', '688', '834', '703', '815', '728', '774', '780', '739', '856', '687', '840', '708', '698', '817', '792', '838', '848', '835', '807', '709', '778'], '774': ['828', '713', '719', '805', '824', '745', '747', '823', '694', '830', '688', '853', '784', '752', '728', '820', '800', '819', '773', '810', '696', '739', '856', '687', '840', '731', '708', '698', '738', '701', '807', '720'], '726': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '697', '724', '688', '853', '834', '695', '784', '815', '752', '820', '773', '766', '780', '754', '760', '731', '845', '701', '778', '844', '693'], '773': ['828', '713', '705', '719', '824', '745', '747', '823', '694', '781', '724', '827', '853', '834', '784', '815', '752', '728', '820', '842', '774', '726', '829', '780', '810', '696', '754', '847', '731', '706', '737', '831'], '766': ['828', '713', '719', '805', '824', '823', '694', '830', '697', '724', '827', '688', '853', '834', '695', '784', '815', '752', '820', '800', '726', '780', '810', '849', '760', '847', '727', '731', '734', '711', '771'], '829': ['828', '713', '705', '719', '805', '824', '745', '694', '781', '724', '853', '834', '695', '784', '752', '842', '773', '696', '760', '847', '814', '738', '734', '711', '795', '777', '822', '722'], '780': ['828', '713', '705', '805', '824', '747', '823', '694', '830', '697', '688', '834', '695', '815', '728', '819', '726', '773', '766', '696', '754', '739', '687', '840', '792', '845', '838', '701', '835', '803', '779', '777', '765'], '810': ['828', '713', '705', '719', '805', '745', '747', '823', '694', '781', '827', '853', '834', '695', '784', '815', '752', '842', '800', '774', '773', '766', '856', '847', '814', '731', '698', '711', '706'], '696': ['828', '713', '719', '805', '824', '745', '747', '823', '694', '830', '697', '724', '688', '853', '834', '695', '784', '842', '774', '773', '829', '780', '739', '840', '727', '738', '845', '711', '795', '778'], '849': ['828', '705', '719', '824', '747', '830', '781', '697', '827', '688', '820', '766', '787', '770', '797', '727', '708', '734', '845', '793', '758', '748', '755', '783', '730', '763'], '718': ['828', '705', '719', '805', '745', '747', '823', '781', '827', '834', '703', '695', '815', '787', '764', '687', '770', '797', '814', '817', '793', '758', '803', '748', '809', '720', '730'], '754': ['828', '705', '805', '824', '747', '823', '694', '830', '697', '724', '688', '703', '695', '815', '728', '726', '773', '780', '739', '840', '792', '838', '701', '848', '835', '779', '777', '709'], '760': ['828', '713', '705', '719', '805', '824', '745', '823', '694', '830', '781', '697', '724', '827', '834', '695', '784', '820', '842', '800', '726', '766', '829', '814', '708', '738', '734', '701', '794'], '739': ['828', '705', '805', '824', '747', '823', '688', '703', '728', '819', '774', '780', '696', '754', '787', '764', '687', '840', '797', '792', '807', '793', '758', '720', '777', '709'], '787': ['713', '705', '719', '745', '823', '781', '827', '703', '728', '849', '718', '739', '764', '797', '814', '817', '793', '758', '803', '748', '755', '783', '809', '720', '730', '763'], '856': ['828', '747', '823', '830', '697', '688', '703', '815', '728', '800', '819', '774', '810', '687', '840', '708', '698', '838', '807', '772', '765'], '764': ['828', '705', '719', '805', '824', '747', '823', '781', '827', '703', '728', '718', '739', '787', '687', '797', '817', '838', '807', '793', '748', '783', '779', '809', '720', '730'], '687': ['828', '805', '747', '823', '830', '697', '688', '703', '728', '820', '819', '774', '780', '718', '739', '856', '764', '797', '817', '838', '807', '783', '779', '777', '730', '765'], '840': ['828', '713', '719', '805', '824', '747', '823', '830', '697', '688', '695', '819', '774', '780', '696', '754', '739', '856', '708', '698', '838', '848', '803', '779', '709'], '847': ['828', '713', '705', '719', '805', '824', '745', '694', '830', '688', '853', '834', '695', '815', '752', '773', '766', '829', '810', '727', '731', '734', '711', '706'], '770': ['828', '705', '824', '747', '781', '688', '820', '849', '718', '797', '727', '734', '845', '803', '748', '783', '777', '772', '741', '763'], '797': ['828', '747', '827', '703', '728', '820', '849', '718', '739', '787', '764', '687', '770', '708', '817', '793', '748', '755', '783', '809', '831', '730', '763'], '814': ['828', '713', '705', '719', '805', '745', '781', '724', '827', '853', '834', '842', '800', '829', '810', '718', '760', '787', '708', '758', '755', '772'], '727': ['828', '719', '805', '824', '694', '830', '697', '688', '853', '784', '752', '842', '800', '766', '696', '849', '847', '770', '845', '848', '711'], '731': ['828', '713', '705', '719', '805', '745', '694', '830', '781', '697', '724', '827', '853', '834', '784', '815', '752', '774', '726', '773', '766', '810', '847', '706'], '708': ['828', '713', '719', '824', '747', '823', '781', '697', '724', '827', '800', '819', '774', '849', '760', '856', '840', '797', '814', '755', '730'], '698': ['828', '713', '719', '745', '747', '823', '697', '703', '800', '819', '774', '810', '856', '840', '803', '772'], '738': ['713', '705', '805', '745', '694', '781', '724', '853', '695', '784', '752', '842', '774', '829', '696', '760', '711', '795', '722', '794'], '817': ['828', '713', '719', '805', '745', '830', '781', '827', '703', '728', '819', '718', '787', '764', '687', '797', '793', '758', '748', '783', '809'], '792': ['828', '805', '824', '747', '823', '830', '688', '703', '815', '728', '819', '780', '754', '739', '838', '835', '807', '803', '779', '765'], '734': ['713', '719', '805', '724', '688', '853', '695', '784', '842', '766', '829', '849', '760', '847', '770', '803', '778', '693'], '845': ['828', '713', '705', '719', '745', '694', '830', '688', '703', '815', '820', '800', '726', '780', '696', '849', '770', '727', '701', '755', '772'], '838': ['828', '805', '824', '747', '823', '830', '703', '815', '728', '819', '780', '754', '856', '764', '687', '840', '792', '835', '807', '779', '709'], '701': ['713', '747', '830', '781', '697', '724', '688', '834', '695', '842', '774', '726', '780', '754', '760', '845', '777', '778', '794'], '848': ['828', '713', '719', '824', '747', '823', '694', '697', '724', '820', '819', '754', '840', '727', '709', '844'], '835': ['828', '705', '824', '823', '697', '703', '815', '728', '819', '780', '754', '792', '838', '779', '741', '709', '765'], '807': ['828', '705', '805', '747', '823', '703', '728', '819', '774', '739', '856', '764', '687', '792', '838', '779', '741', '765'], '793': ['747', '781', '703', '849', '718', '739', '787', '764', '797', '817', '758', '748', '755', '783', '809', '720'], '758': ['705', '745', '747', '781', '827', '703', '849', '718', '739', '787', '814', '817', '793', '748', '755', '783', '809'], '711': ['828', '745', '694', '724', '853', '695', '784', '752', '842', '766', '829', '810', '696', '847', '727', '738', '795', '722'], '803': ['705', '747', '728', '820', '780', '718', '787', '840', '770', '698', '792', '734', '720', '730', '763'], '748': ['713', '705', '747', '830', '697', '703', '849', '718', '787', '764', '770', '797', '817', '793', '758', '779', '831'], '755': ['705', '719', '745', '781', '827', '849', '787', '797', '814', '708', '845', '793', '758', '783', '809', '720'], '783': ['747', '781', '827', '728', '820', '849', '787', '764', '687', '770', '797', '817', '793', '758', '755', '809', '730'], '779': ['828', '824', '823', '703', '728', '780', '754', '764', '687', '840', '792', '838', '835', '807', '748', '777', '765'], '809': ['705', '747', '781', '827', '703', '728', '718', '787', '764', '797', '817', '793', '758', '755', '783', '720'], '720': ['705', '719', '745', '781', '774', '718', '739', '787', '764', '793', '803', '755', '809', '730'], '706': ['828', '713', '705', '719', '805', '745', '694', '853', '834', '695', '784', '815', '752', '773', '810', '847', '731'], '795': ['745', '694', '724', '853', '784', '820', '842', '829', '696', '738', '711', '794'], '777': ['713', '824', '830', '697', '724', '688', '695', '829', '780', '754', '739', '687', '770', '701', '779', '709'], '772': ['828', '713', '705', '719', '805', '694', '781', '697', '827', '688', '800', '856', '770', '814', '698', '845'], '741': ['828', '705', '747', '823', '781', '827', '703', '728', '820', '800', '770', '835', '807', '763'], '709': ['828', '805', '824', '823', '830', '697', '703', '819', '754', '739', '840', '838', '848', '835', '777'], '818': ['713', '805', '697', '822', '806', '839', '771', '732', '736', '714'], '778': ['828', '713', '705', '830', '724', '834', '695', '819', '726', '696', '734', '701', '844', '693'], '737': ['828', '705', '719', '805', '824', '745', '694', '830', '815', '728', '820', '773', '722'], '831': ['713', '705', '805', '747', '694', '853', '703', '752', '820', '773', '797', '748'], '833': ['713', '806', '768', '736'], '822': ['829', '818', '806', '839', '768', '732', '714'], '806': ['818', '833', '822', '839', '771', '732', '714'], '730': ['747', '703', '728', '849', '718', '787', '764', '687', '797', '708', '803', '783', '720', '763'], '839': ['818', '822', '806', '771', '768', '732', '714'], '771': ['688', '766', '818', '806', '839', '732', '714'], '722': ['713', '705', '719', '745', '694', '853', '784', '820', '829', '738', '711', '737', '768'], '844': ['828', '713', '705', '805', '824', '830', '697', '834', '695', '726', '848', '778', '693'], '768': ['719', '833', '822', '839', '722'], '732': ['818', '822', '806', '839', '771', '714'], '736': ['713', '724', '818', '833'], '794': ['713', '719', '745', '781', '724', '695', '784', '842', '760', '738', '701', '795'], '693': ['828', '713', '705', '824', '823', '697', '834', '695', '726', '734', '778', '844'], '763': ['705', '747', '820', '849', '787', '770', '797', '803', '741', '730'], '765': ['747', '823', '697', '703', '728', '780', '856', '687', '792', '835', '807', '779'], '714': ['818', '822', '806', '839', '771', '732']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = nx.read_gml('new_facebook_network.gml')\n",
    "neighbors = {}\n",
    "\n",
    "for node in g.nodes:\n",
    "    neighbors[node] = list(g.neighbors(node))\n",
    "\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read new_facebook_network.gml and create a fully connected graph\n",
    "\n",
    "neighbors_fully_connected = {}\n",
    "\n",
    "for node in g.nodes:\n",
    "    neighbors_fully_connected[node] = list(g.nodes)\n",
    "    neighbors_fully_connected[node].remove(node)\n",
    "\n",
    "    \n",
    "# print(neighbors_fully_connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[18, 1433], edge_index=[2, 58], y=[18])\n",
      "torch.Size([18, 1433])\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(sub_data_list[0])\n",
    "print(sub_data_list[0].x.shape)\n",
    "print(type(client_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep a  list of training and validation loss per epoch for each subgraph\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def transductive_split(data, train_percent=0.8, val_percent=0.1):\n",
    "    \"\"\"\n",
    "    Split graph data into training, validation, and testing sets for transductive learning.\n",
    "    :param data: PyG Data object\n",
    "    :param train_percent: Percentage of nodes to be used for training\n",
    "    :param val_percent: Percentage of nodes to be used for validation\n",
    "    :return: data object with train_mask, val_mask, and test_mask attributes added\n",
    "    \"\"\"\n",
    "    # set a seed for reproducibility\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    train_size = int(train_percent * num_nodes)\n",
    "    val_size = int(val_percent * num_nodes)\n",
    "\n",
    "    # Create a random permutation of node indices\n",
    "    perm = torch.randperm(num_nodes)\n",
    "\n",
    "    # Create masks for training, validation, and testing nodes\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[perm[:train_size]] = True\n",
    "    val_mask[perm[train_size:train_size + val_size]] = True\n",
    "    test_mask[perm[train_size + val_size:]] = True\n",
    "\n",
    "    # Add masks to data object\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the subgraphs into train test and val \n",
    "# for i in range(0, 100):\n",
    "#     sub_data = sub_data_list[i]\n",
    "#     sub_data = transductive_split(sub_data)\n",
    "\n",
    "\n",
    "# print(torch.sum(sub_data_list[4].train_mask).item())  # Number of training nodes\n",
    "# print(torch.sum(sub_data_list[4].val_mask).item())    # Number of validation nodes\n",
    "# print(torch.sum(sub_data_list[4].test_mask).item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n",
    "    \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training) # p = 0.25\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, axs = plt.subplots(10, 10, figsize=(50, 50))\n",
    "\n",
    "def train(sub_data, model, optimizer, criterion):\n",
    "      model.train()\n",
    "      optimizer.zero_grad() \n",
    "      out = model(sub_data.x, sub_data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out, sub_data.y)  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward() \n",
    "      optimizer.step() \n",
    "      return loss\n",
    "\n",
    "def test(test_data, criterion, model):\n",
    "      model.eval()\n",
    "      out = model(test_data.x, test_data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred == test_data.y # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / len(test_data.y)  # Derive ratio of correct predictions.\n",
    "      test_loss = criterion(out, test_data.y)  # Compute validation loss\n",
    "      \n",
    "      return test_loss, test_acc\n",
    "\n",
    "\n",
    "def validate(test_data, model, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Do not compute gradients during this step\n",
    "        out = model(test_data.x, test_data.edge_index)  # Forward pass\n",
    "        pred = out.argmax(dim=1)  # Get predicted classes\n",
    "        val_correct = pred == test_data.y # Compare with ground-truth\n",
    "        val_loss = criterion(out, test_data.y)  # Compute validation loss\n",
    "        val_acc = int(val_correct.sum()) / int(len(test_data.y))  # Compute validation accuracy\n",
    "    return val_loss.item(), val_acc  # Return validation loss and accuracy\n",
    "\n",
    "\n",
    "def initial_training(epochs):\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    dictionary_t_1 = {}\n",
    "    dictionary_t = {}\n",
    "\n",
    "    for i in range(len(sub_data_list)):\n",
    "        sub_data = sub_data_list[i]\n",
    "        \n",
    "        model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        train_losses = [] \n",
    "        val_losses = []    \n",
    "\n",
    "        # Training loop for each epoch (adjust the range as needed)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = train(sub_data, model, optimizer, criterion)  # Assuming 'train' returns a loss\n",
    "            val_loss, _ = validate(test_data, model, criterion)         # Assuming 'validate' returns a loss and something else (like accuracy)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "        \n",
    "        test_loss, test_acc = test(test_data, criterion, model)\n",
    "        test_losses.append(test_loss.item())\n",
    "        test_accs.append(test_acc)\n",
    "        row = i // 10\n",
    "        col = i % 10\n",
    "\n",
    "        model_weights = model.state_dict()\n",
    "        client_number_num = client_number[i]\n",
    "        dictionary_t_1[client_number_num] = model_weights\n",
    "        \n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    return dictionary_t_1, test_losses, test_accs\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  2.3276490342617033\n",
      "mean test accuracy before communication:  0.14737142857142846\n"
     ]
    }
   ],
   "source": [
    "dictionary_t_1, test_losses, test_accs = initial_training(10)\n",
    "\n",
    "# average test loss and accuracy before communication \n",
    "\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(test_accs)\n",
    "plt.title(\"Test accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg(models):\n",
    "    global_model = copy.deepcopy(models[0]) # start with the first model\n",
    "    for k in global_model.keys():\n",
    "        global_model[k] = torch.stack([model[k].float() for model in models], 0).mean(0)\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communication(dictionary_t_1_og, neighbors, epochs, rounds):\n",
    "    # make a copy of dictionary_t_1\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    dictionary_t_1 = dictionary_t_1_og.copy()\n",
    "    dictionary_t = {}\n",
    "    for k in range(1, rounds + 1):\n",
    "\n",
    "    # get the neighbprs of each client\n",
    "        for i in range(len(sub_data_list)):\n",
    "            sub_data = sub_data_list[i]\n",
    "            \n",
    "            model = GCN(sub_data.num_node_features, dataset.num_classes).to(device)\n",
    "            model.load_state_dict(dictionary_t_1[client_number[i]])\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            train_losses = [] \n",
    "            val_losses = []    \n",
    "\n",
    "            # Training loop for each epoch (adjust the range as needed)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss = train(sub_data, model, optimizer, criterion)  # Assuming 'train' returns a loss\n",
    "                val_loss, _ = validate(test_data, model, criterion)         # Assuming 'validate' returns a loss and something else (like accuracy)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            \n",
    "            test_loss, test_acc = test(test_data, criterion, model)\n",
    "            if k == rounds:\n",
    "                test_losses.append(test_loss.item())\n",
    "                test_accs.append(test_acc)\n",
    "\n",
    "        \n",
    "            # add the model weights to the t -1 dictionary\n",
    "            model_weights = model.state_dict()\n",
    "            client_number_num = client_number[i]\n",
    "            dictionary_t_1[client_number_num] = model_weights\n",
    "        \n",
    "\n",
    "        for i in range(100):\n",
    "            client_number_num = client_number[i]\n",
    "            # go through neighbors of client_number\n",
    "            string_client_num = str(client_number_num)\n",
    "            client_neighbors = neighbors[string_client_num]\n",
    "            \n",
    "            neighbors_state_dicts = []\n",
    "            neighbors_state_dicts.append(dictionary_t_1[client_number_num])\n",
    "            for j in range(len(client_neighbors)):\n",
    "                # get model weights of neighbor\n",
    "                neighbor_model = dictionary_t_1[int(client_neighbors[j])]\n",
    "                neighbors_state_dicts.append(neighbor_model)\n",
    "            \n",
    "            #call fed_avg\n",
    "            average_state_dict = {}\n",
    "            average_state_dict = fed_avg(neighbors_state_dicts)\n",
    "            # average weights of client and neighbors\n",
    "            # average_state_dict = {}\n",
    "            # for param in neighbors_state_dicts[0]:\n",
    "            #     # num parameters\n",
    "            #     num_neighbors = len(neighbors_state_dicts)\n",
    "            #     sum_param = 0\n",
    "            #     for neighbor in range(num_neighbors):\n",
    "            #         sum_param += neighbors_state_dicts[neighbor][param]\n",
    "            #     average_param = sum_param / num_neighbors\n",
    "            #     average_state_dict[param] = average_param\n",
    "                \n",
    "                \n",
    "            dictionary_t[client_number_num] = average_state_dict    \n",
    "            \n",
    "        dictionary_t_1 = dictionary_t\n",
    "\n",
    "    return test_losses, test_accs\n",
    " \n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss after communication:  3.1825060403347014\n",
      "mean test accuracy after communication:  0.14697142857142848\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 10, 10)\n",
    "\n",
    "# average test loss and accuracy after communication\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "\n",
    "#mean of test_accs\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print a distriution of the accuracies of test_losses and test_accs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(test_accs)\n",
    "plt.title(\"Test accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 epoch, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  1.9471930146217347\n",
      "mean test accuracy before communication:  0.15068571428571437\n",
      "mean test loss after communication:  2.0834278416633607\n",
      "mean test accuracy after communication:  0.1428571428571427\n"
     ]
    }
   ],
   "source": [
    "# 1 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(1)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors_fully_connected, 1, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  2.021461166143417\n",
      "mean test accuracy before communication:  0.14885714285714272\n",
      "mean test loss after communication:  2.9431652987003325\n",
      "mean test accuracy after communication:  0.15485714285714278\n"
     ]
    }
   ],
   "source": [
    "# 5 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(5)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors_fully_connected, 5, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  2.38707888007164\n",
      "mean test accuracy before communication:  0.14731428571428562\n",
      "mean test loss after communication:  5.009590651988983\n",
      "mean test accuracy after communication:  0.14902857142857132\n"
     ]
    }
   ],
   "source": [
    "# 01 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(10)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors_fully_connected, 10, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Social Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 epoch, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  1.9474929463863373\n",
      "mean test accuracy before communication:  0.15131428571428573\n",
      "mean test loss after communication:  1.9964843916893005\n",
      "mean test accuracy after communication:  0.14311428571428558\n"
     ]
    }
   ],
   "source": [
    "# 1 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(1)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 1, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  2.028321679830551\n",
      "mean test accuracy before communication:  0.14379999999999987\n",
      "mean test loss after communication:  2.730417720079422\n",
      "mean test accuracy after communication:  0.16162857142857132\n"
     ]
    }
   ],
   "source": [
    "# 5 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(5)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 5, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  2.321071753501892\n",
      "mean test accuracy before communication:  0.14737142857142846\n",
      "mean test loss after communication:  4.401576869487762\n",
      "mean test accuracy after communication:  0.16074285714285705\n"
     ]
    }
   ],
   "source": [
    "# 10 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(10)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 10, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 epochs, 100 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  3.1005760717391966\n",
      "mean test accuracy before communication:  0.15391428571428561\n",
      "mean test loss after communication:  4.653899253606796\n",
      "mean test accuracy after communication:  0.1849999999999999\n"
     ]
    }
   ],
   "source": [
    "# 20 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(20)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 20, 100)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 epochs, 10 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  1.9460879898071288\n",
      "mean test accuracy before communication:  0.1540285714285714\n",
      "mean test loss after communication:  1.9453727030754089\n",
      "mean test accuracy after communication:  0.15122857142857135\n"
     ]
    }
   ],
   "source": [
    "# 1 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(1)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 1, 10)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 epochs, 10 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  2.01352867603302\n",
      "mean test accuracy before communication:  0.14522857142857135\n",
      "mean test loss after communication:  2.281940995454788\n",
      "mean test accuracy after communication:  0.144942857142857\n"
     ]
    }
   ],
   "source": [
    "# 5 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(5)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 5, 10)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 epochs, 10 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test loss before communication:  2.3487125921249388\n",
      "mean test accuracy before communication:  0.14694285714285704\n",
      "mean test loss after communication:  3.083989210128784\n",
      "mean test accuracy after communication:  0.15011428571428553\n"
     ]
    }
   ],
   "source": [
    "# 10 epoch\n",
    "\n",
    "dictionary_t_1, test_losses, test_accs = initial_training(10)\n",
    "# print average test loss and average test accs\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss before communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy before communication: \", mean_test_accs)\n",
    "\n",
    "test_losses, test_accs = communication(dictionary_t_1, neighbors, 10, 10)\n",
    "mean_test_losses = sum(test_losses)/len(test_losses)\n",
    "mean_test_accs = sum(test_accs)/len(test_accs)\n",
    "print(\"mean test loss after communication: \", mean_test_losses)\n",
    "print(\"mean test accuracy after communication: \", mean_test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Graph Results\n",
    "### (Training with original subgraphs)\n",
    "\n",
    "| Epochs | Communication Rounds | Avg Test Accuracy (Before) | Avg Test Accuracy (After) | Avg Test Loss (Before) | Avg Test Loss (After) |\n",
    "| ------ | -------------------- | -------------------------- | ------------------------- | ---------------------- | --------------------- |\n",
    "| 1      | 100                  | 15.07%                     | 14.29%                    | 1.947                  | 2.083                 |\n",
    "| 5      | 100                  | 14.89%                     | 15.49%                    | 2.021                  | 2.943                 |\n",
    "| 10     | 100                  | 14.73%                     | 14.90%                    | 2.387                  | 5.010                 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Network Graph Results\n",
    "### (Training with original subgraphs)\n",
    "\n",
    "| Epochs | Communication Rounds | Avg Test Accuracy (Before) | Avg Test Accuracy (After) | Avg Test Loss (Before) | Avg Test Loss (After) |\n",
    "| ------ | -------------------- | -------------------------- | ------------------------- | ---------------------- | --------------------- |\n",
    "| 1      | 100                  | 15.13%                     | 14.31%                    | 1.947                  | 1.996                 |\n",
    "| 1      | 10                   | 15.40%                     | 15.12%                    | 1.946                  | 1.945                 |\n",
    "| 5      | 100                  | 14.38%                     | 16.16%                    | 2.028                  | 2.730                 |\n",
    "| 5      | 10                   | 14.52%                     | 14.49%                    | 2.014                  | 2.282                 |\n",
    "| 10     | 100                  | 14.74%                     | 16.07%                    | 2.321                  | 4.402                 |\n",
    "| 10     | 10                   | 14.69%                     | 15.01%                    | 2.349                  | 3.084                 |\n",
    "| 20     | 100                  | 15.39%                     | 18.50%                    | 3.101                  | 4.654                 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
